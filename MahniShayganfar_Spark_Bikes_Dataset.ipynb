{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb44b2e",
   "metadata": {},
   "source": [
    "# Regression with gradient-boosted trees and MLlib pipelines\n",
    "\n",
    "This notebook uses a bike-sharing dataset to illustrate MLlib pipelines and the gradient-boosted trees machine learning algorithm. The challenge is to predict the number of bicycle rentals per hour based on the features available in the dataset such as day of the week, weather, season, and so on. Demand prediction is a common problem across businesses; good predictions allow a business or service to optimize inventory and to match supply and demand to make customers happy and maximize profitability.\n",
    "\n",
    "### Load the dataset\n",
    "\n",
    "The dataset is from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) and is provided with Databricks Runtime. The dataset includes information about bicycle rentals from the Capital bikeshare system in 2011 and 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381a4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67103cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('bikes').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c24ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[instant: int, dteday: timestamp, season: int, yr: int, mnth: int, hr: int, holiday: int, weekday: int, workingday: int, weathersit: int, temp: double, atemp: double, hum: double, windspeed: double, casual: int, registered: int, cnt: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/hour.csv\", header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "# The following command caches the DataFrame in memory. \n",
    "# This improves performance since subsequent calls to the DataFrame can read from memory instead of re-reading the \n",
    "# data from disk.\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf2c52d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "|instant|             dteday|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|casual|registered|cnt|\n",
      "+-------+-------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "|      1|2011-01-01 00:00:00|     1|  0|   1|  0|      0|      6|         0|         1|0.24|0.2879|0.81|      0.0|     3|        13| 16|\n",
      "|      2|2011-01-01 00:00:00|     1|  0|   1|  1|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|     8|        32| 40|\n",
      "|      3|2011-01-01 00:00:00|     1|  0|   1|  2|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|     5|        27| 32|\n",
      "|      4|2011-01-01 00:00:00|     1|  0|   1|  3|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     3|        10| 13|\n",
      "|      5|2011-01-01 00:00:00|     1|  0|   1|  4|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     0|         1|  1|\n",
      "+-------+-------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd346b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 17379 rows.\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset has %d rows.\" % df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8401ba",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "This dataset is well prepared for machine learning algorithms. The numeric input columns (temp, atemp, hum, and windspeed) are normalized, categorial values (season, yr, mnth, hr, holiday, weekday, workingday, weathersit) are converted to indices, and all of the columns except for the date (dteday) are numeric.\n",
    "\n",
    "The goal is to predict the count of bike rentals (the cnt column). Reviewing the dataset, you can see that some columns contain duplicate information. For example, the cnt column equals the sum of the casual and registered columns. We should remove the casual and registered columns from the dataset. The index column instant is also not useful as a predictor.\n",
    "\n",
    "We can also delete the column dteday, as this information is already included in the other date-related columns yr, mnth, and weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45a59db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+---+\n",
      "|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|cnt|\n",
      "+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+---+\n",
      "|     1|  0|   1|  0|      0|      6|         0|         1|0.24|0.2879|0.81|      0.0| 16|\n",
      "|     1|  0|   1|  1|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0| 40|\n",
      "|     1|  0|   1|  2|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0| 32|\n",
      "|     1|  0|   1|  3|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0| 13|\n",
      "|     1|  0|   1|  4|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|  1|\n",
      "+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(\"instant\").drop(\"dteday\").drop(\"casual\").drop(\"registered\")\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe96d6",
   "metadata": {},
   "source": [
    "Print the dataset schema to see the type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3231a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: integer (nullable = true)\n",
      " |-- yr: integer (nullable = true)\n",
      " |-- mnth: integer (nullable = true)\n",
      " |-- hr: integer (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- weathersit: integer (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- cnt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5940c5",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "Randomly split data into training and test sets. By doing this, we can train and tune the model using only the training subset, and then evaluate the model's performance on the test set to get a sense of how the model will perform on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a1841ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12234 training examples and 5145 test examples.\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset randomly into 70% for training and 30% for testing. Passing a seed for deterministic behavior\n",
    "\n",
    "train, test = df.randomSplit([0.7, 0.3], seed = 42)\n",
    "\n",
    "print(\"There are %d training examples and %d test examples.\" % (train.count(), test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa4e77",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "We can plot the data to explore it visually. The following plot shows the number of bicycle rentals during each hour of the day. As we might expect, rentals are low during the night, and peak at commute hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = (train.select(\"hr\", \"cnt\")).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b554f5",
   "metadata": {},
   "source": [
    "### Train the machine learning pipeline\n",
    "\n",
    "Now that we have reviewed the data and prepared it as a DataFrame with numeric values, we're ready to train a model to predict future bike sharing rentals.\n",
    "\n",
    "Most MLlib algorithms require a single input column containing a vector of features and a single target column. The DataFrame currently has one column for each feature. MLlib provides functions to help us prepare the dataset in the required format.\n",
    "\n",
    "MLlib pipelines combine multiple steps into a single workflow, making it easier to iterate as you develop the model.\n",
    "\n",
    "In this example, we create a pipeline using the following functions:\n",
    "\n",
    "* VectorAssembler: Assembles the feature columns into a feature vector.\n",
    "* VectorIndexer: Identifies columns that should be treated as categorical. This is done heuristically, identifying any column with a small number of distinct values as categorical. In this example, the following columns are considered categorical: yr (2 values), season (4 values), holiday (2 values), workingday (2 values), and weathersit (4 values).\n",
    "* GBTRegressor: Uses the [Gradient-Boosted Trees (GBT)](https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-classifier) algorithm to learn how to predict rental counts from the feature vectors.\n",
    "* CrossValidator: The GBT algorithm has several hyperparameters. This notebook illustrates how to use [hyperparameter tuning in Spark](https://spark.apache.org/docs/latest/ml-tuning.html). This capability automatically tests a grid of hyperparameters and chooses the best resulting model.\n",
    "\n",
    "For more information:\n",
    "[VectorAssembler](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler)\n",
    "[VectorIndexer](https://spark.apache.org/docs/latest/ml-features.html#vectorindexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad46a8b",
   "metadata": {},
   "source": [
    "The first step is to create the VectorAssembler and VectorIndexer steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18b40d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed82ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the target column from the input feature set.\n",
    "featuresCols = df.columns\n",
    "featuresCols.remove('cnt')\n",
    " \n",
    "# vectorAssembler combines all feature columns into a single feature vector column, \"rawFeatures\".\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    " \n",
    "# vectorIndexer identifies categorical features and indexes them, and creates a new column \"features\". \n",
    "vectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed117d",
   "metadata": {},
   "source": [
    "Next, we define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502cf707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d94f3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to define the model training stage of the pipeline. \n",
    "# The following command defines a GBTRegressor model that takes an input column \"features\" by default and learns to predict the labels in the \"cnt\" column. \n",
    "\n",
    "gbt = GBTRegressor(labelCol=\"cnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296abc27",
   "metadata": {},
   "source": [
    "The third step is to wrap the model you just defined in a CrossValidator stage. CrossValidator calls the GBT algorithm with different hyperparameter settings. It trains multiple models and selects the best one, based on minimizing a specified metric. In this example, the metric is root mean squared error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89d8c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19700a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameters to test:\n",
    "#  - maxDepth: maximum depth of each decision tree \n",
    "#  - maxIter: iterations, or the total number of trees \n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(gbt.maxDepth, [2, 5])\\\n",
    "  .addGrid(gbt.maxIter, [10, 100])\\\n",
    "  .build()\n",
    " \n",
    "# Define an evaluation metric. The CrossValidator compares the true labels with predicted values for each \n",
    "# combination of parameters, and calculates this value to determine the best model.\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())\n",
    " \n",
    "# Declare the CrossValidator, which performs the model tuning.\n",
    "cv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25dfd6f",
   "metadata": {},
   "source": [
    "We create the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16851b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a00f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe8495",
   "metadata": {},
   "source": [
    "We train the pipeline.\n",
    "\n",
    "Now that we have set up the workflow, we can train the pipeline with a single call.\n",
    "\n",
    "When we call fit(), the pipeline runs feature processing, model tuning, and training and returns a fitted pipeline with the best model it found. This step takes several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "271fa61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c124710",
   "metadata": {},
   "source": [
    "### Make predictions and evaluate results\n",
    "\n",
    "The final step is to use the fitted model to make predictions on the test dataset and evaluate the model's performance. The model's performance on the test dataset provides an approximation of how it is likely to perform on new data. For example, if we had weather predictions for the next week, we could predict bike rentals expected during the next week.\n",
    "\n",
    "Computing evaluation metrics is important for understanding the quality of predictions, as well as for comparing models and tuning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa35c5f3",
   "metadata": {},
   "source": [
    "The transform() method of the pipeline model applies the full pipeline to the input dataset. The pipeline applies the feature processing steps to the dataset and then uses the fitted GBT model to make predictions. The pipeline returns a DataFrame with a new column predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e10ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ec5f668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+\n",
      "|cnt|        prediction|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|\n",
      "+---+------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+\n",
      "| 33|22.358993226174217|     1|  0|   1|  0|      0|      0|         0|         1|0.16|0.1818| 0.8|   0.1045|\n",
      "|  5| 7.334803031501155|     1|  0|   1|  0|      0|      1|         1|         1|0.12|0.1212| 0.5|   0.2836|\n",
      "|  7| 18.12532344601902|     1|  0|   1|  0|      0|      1|         1|         2|0.24|0.2273|0.65|   0.2239|\n",
      "| 12| 5.966541811759386|     1|  0|   1|  0|      0|      2|         1|         1|0.14|0.1667|0.59|   0.1045|\n",
      "|  7| 8.459601443664615|     1|  0|   1|  0|      0|      3|         1|         2|0.16| 0.197|0.86|   0.0896|\n",
      "| 17|15.370219551935245|     1|  0|   1|  0|      0|      3|         1|         2|0.22|0.2273|0.69|   0.1343|\n",
      "|  3| 9.948241628003927|     1|  0|   1|  0|      0|      3|         1|         2|0.22|0.2727|0.93|      0.0|\n",
      "| 14| 7.832165488057728|     1|  0|   1|  0|      0|      5|         1|         1|0.12|0.1364| 0.5|    0.194|\n",
      "|  9|15.403109759973406|     1|  0|   1|  0|      0|      5|         1|         2| 0.2|0.2121|0.75|   0.1343|\n",
      "| 13|28.455496586198326|     1|  0|   1|  0|      0|      6|         0|         1|0.04|0.0303|0.45|   0.2537|\n",
      "+---+------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"cnt\", \"prediction\", *featuresCols).show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786b760",
   "metadata": {},
   "source": [
    "A common way to evaluate the performance of a regression model is the calculate the root mean squared error (RMSE). The value is not very informative on its own, but we can use it to compare different models. CrossValidator determines the best model by selecting the one that minimizes RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1115da2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on our test set: 45.1486\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"RMSE on our test set: %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ebaa1",
   "metadata": {},
   "source": [
    "We can also plot the results. In this case, the hourly count of rentals shows a similar shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb72b05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hr: int, prediction: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predictions.select(\"hr\", \"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea7ade",
   "metadata": {},
   "source": [
    "It's also a good idea to examine the residuals, or the difference between the expected result and the predicted value. \n",
    "\n",
    "The residuals should be randomly distributed; if there are any patterns in the residuals, the model may not be capturing something important. In this case, the average residual is about 0.5, less than 0.5% of the average value of the cnt column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b843a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48097dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(residual)|\n",
      "+------------------+\n",
      "|0.4479244253106421|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_with_residuals = predictions.withColumn(\"residual\", (F.col(\"cnt\") - F.col(\"prediction\")))\n",
    "\n",
    "predictions_with_residuals.agg({'residual': 'mean'}).show(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f39de",
   "metadata": {},
   "source": [
    "We can plot the residuals across the hours of the day to look for any patterns. In this example, there are no obvious correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1b76743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_with_residuals.select(\"hr\", \"residual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c24febc",
   "metadata": {},
   "source": [
    "### Improving the model\n",
    "\n",
    "Here are some suggestions for improving this model:\n",
    "\n",
    "* The count of rentals is the sum of registered and casual rentals. These two counts may have different behavior, as frequent cyclists and casual cyclists may rent bikes for different reasons. We can try training one GBT model for registered and one for casual, and then add their predictions together to get the full prediction.\n",
    "\n",
    "* For efficiency, this notebook used only a few hyperparameter settings. We might be able to improve the model by testing more settings. A good start is to increase the number of trees by setting maxIter=200; this takes longer to train but might more accurate.\n",
    "\n",
    "* This notebook used the dataset features as-is, but we might be able to improve performance with some feature engineering. For example, the weather might have more of an impact on the number of rentals on weekends and holidays than on workdays. We could try creating a new feature by combining those two columns. MLlib provides a suite of feature transformers; find out more in the ML guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799684e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
